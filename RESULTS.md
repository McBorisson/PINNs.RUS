### Итоги:
* Проблема с z0 была в функции активации(была tanh, теперь будет sin)(см exp3), остальные изменения заметных положительных эффектов не дали(cм exp4, exp5).
* Решение всё ещё плохо продолжается на большие t, а если и продолжается, то почему-то идёт ниже, чем нужно(см exp6).
* Soft Adapt тоже не очень помог, коэффициенты просто постепенно приравниваются(см exp7).
* Что интересно, при lr=1e-4 и ниже сеть вроде учится, loss уменьшается, но на выходе получается какая-то странная фигня, не похожая ни на константу ни на аналитическое решение(см exp8).
---
* Чтобы получить хорошие результаты нужно было поправить процесс обучения и использовать 100000 итераций оптимизатора ADAM с убывающим lr(см exp9)
* Использование балансировки весов увеличивает количество необходимых итераций, но в то же время улучшает стабильность уменьшения mse_q. Причём чем сильнее разбиение, тем сильнее эффект(см exp10, exp11)
---
Статистику обучения в exp9, exp10, exp11 и сравнительные графики можно найти в файле: [stats.xlsx](https://github.com/mikhakuv/PINNs/blob/main/stats.xlsx)   
Результаты всех успешных экспериментов можно найти в таблице: [performance_table.xlsx](https://docs.google.com/spreadsheets/d/1EAHA_UamNzLTHufkJSFcJTIRn0lpgeh28o-bcWYOjjE/edit?usp=sharing)
