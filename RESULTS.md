# Теория  
В данной работе применяется метод PINN для решения нелинейного дифференциального уравнения второго порядка, а полученный результат сравнивается с аналитическим решением. Исследуется влияение методов балансировки весов на качество обучения.
### Уравнение
Рассматривается обобщённое уравнение Шрёдингера в нелинейной среде (как ещё перевести generalized Schrodinger equation with a dual-power law nonlinear medium?):
$$iq_t + aq_{xx} + \alpha|q|^{2n}q - \beta|q|^{4n}q = 0$$
В [[1]](#обзор-литературы) найдено аналитическое решение такого уравнения:
$$q(x, t)=\left[\frac{4 \mu \mathrm{e}^{\left(x-2 a k t-z_0\right) \sqrt{\mu}}}{1+2 \lambda \mathrm{e}^{\left(x-2 a k t-z_0\right) \sqrt{\mu}}+\left(\lambda^2-4 \mu \nu\right) \mathrm{e}^{2\left(x-2 a k t-z_0\right) \sqrt{\mu}}}\right]^{\frac{1}{2 n}} \mathrm{e}^{i\left(k x-\omega t+\theta_0\right)},$$
$$где\quad \lambda=\frac{4 \alpha n^2}{a(1+n)},\quad \nu=\frac{4 \beta n^2}{a(1+2 n)},\quad \mu=\frac{4 n^2(\omega-a k^2)}{a} .$$
рассматривается  случай $\quad n = 1,\quad a = 1,\quad \alpha = 1,\quad \beta = 1,\quad$ при котором $\quad k = 1,\quad z_0 = 0,\quad \theta_0 = 0,\quad w = \frac{9}{8}.$
### PINN
Решение уравнения находится в виде нейросети. Это возможно потому, что нейросеть рассматривается как функция, а от функции можно считать производные разных порядков и из них составить исходное уравнение. Полученное уравнение будет использоваться как первое слагаемое `loss` (далее обозначается как `loss_f`). Задачей оптимизатора будет как можно сильнее уменьшить `loss`, а значит и заставить нейросеть удовлетворять уравнению. Чтобы в итоге не получалось тривиальное решение, вторым слагаемым `loss` будет ошибка выполнения начальных и граничных условий (далее обозначается как `loss_uv`). В данной работе начальное условие определяется как значения аналитического решения в $(x,t_0)$, а граничные условия считаются нулевыми в $(x_0,t)$ и $(x_1,t)$. В итоге минимизируется следующая функция: `loss = loss_f + loss_uv`. Весь процесс изображён на картинке(не моя если что):  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/results_illustration.png">  
Такой подход называется **P**hysics **I**nformed **N**eural **N**etwork и был впервые представлен в [[2]](#обзор-литературы). Он существенно отличается от численных методов тем, что в итоге получается не массив чисел, а дифференцируемая функция.  
### Балансировка весов
Несмотря на то, что метод работает неплохо, есть много попыток улучшить его. Один из наиболее распространённых приёмов - динамическая балансировка весов, она используется не только в PINN, но и в других областях. Идея заключается в том, чтобы нейросеть одинаково хорошо училась удовлетворять всем условиям. Для этого при слагаемых `loss` добавляются коэффициенты и в итоге он имеет вид: $$loss = \sum_i lambda_i * loss_i$$ Есть несколько методов вычисления коэффициентов в процессе обучения, в данной работе изучается и совершенствуется метод SoftAdapt, предложенный в статье [[3]](#обзор-литературы). Изначально он применялся к задаче реконструкции изображений и использовал относительность прогресса в уменьшении составных частей loss. Обозначим как $loss_i(iter)$ значение $i$-того слагаемого `loss` на шаге $iter$. Тогда коэффициент $lambda_i$ при слагаемом $loss_i$ определяется по формуле: $$lambda_i = \frac{exp(\frac{loss_i(iter)}{loss_i(iter-1)})}{\sum_j exp(\frac{loss_j(iter)}{loss_j(iter-1)})}$$ Смысл такой формулы простой: во-первых коэффициент при $loss_i$ снижается, если он($loss_i$) уменьшился сильнее других и повышается в обратном случае, во-вторых сумма всех коэффициентов равна 1. Ясно, что на следующем шаге оптимизатору будет выгоднее снижать $loss_i$, у которого коэффициент $lambda_i$ больше и поэтому в процессе обучения все $loss_i$ будут снижены одинаково хорошо.  
Можно разбивать `loss` не только на 2, но и на большее число слагаемых за счёт разделения граничных и начальных условий, а также действительной и мнимой частей. В данной работе будет проверена работа метода SoftAdapt на 2, 3 и 6 слагаемых.  
Также можно усовершенствовать текущий метод, добавив ему память предыдущих итераций. Это можно сделать, если присваивать значения коэффициентов $lambda_i$ следующим образом: $$lambda_i = \tau*lambda_i(iter-1) + (1-\tau)*lambda_i(iter)$$ где $\tau \in[0,1]$ - коэффициент, обозначающий затухание. Чем ближе $\tau$ к 1, тем больше влияние предыдущих значений $lambda_i$.  
# Методика измерений и результаты  
### Параметры нейросети
В опытах использовалась нейросеть с топологией [2,100,100,100,100,2]: 2 входа - переменные $x$ и $t$; дальше идут 4 полносвязных слоя по 100 нейронов в каждом; 2 выхода - действительная($u$) и мнимая($v$) части. Обучение происходит в 2 этапа: сначала ~100000 итераций оптимизатора Adam, потом включается LBFGS. Функция активации нейронов - sin. Используемые параметры получены методом перебора как наиболее подходящие.
### Методика измерения ошибок
Мерой успеха считалось значение функции $mse_q$ , вычисляемое как: $$mse_q=\frac{\sum_{1 \leq i \leq n}(\sqrt{u_{truth}(x_i,t_i)^2 +v_{truth}(x_i,t_i)^2} - \sqrt{u_{pred}(x_i,t_i)^2 +v_{pred}(x_i,t_i)^2})^2}{n}$$ где $n$ - количество рассматриваемых точек на области  
Также вычислялись $mse_u$, $mse_v$, $mse_{f_u}$, $mse_{f_v}$
### Результаты
Полученные результаты изображены на графиках 1 и 2:(пока не добавил)
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/results_chart1.png">  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/results_chart2.png">  
# Обзор Литературы  
1. вроде ещё неопубликованная статья *Nikolay A. Kudryashov, Daniil R. Nifontov* "Application of machine learning to construct solutions to non-integrable partial differential equations"
2. *Maziar Raissi, Paris Perdikaris, George Em Karniadakis* "Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations"
3. *A. Ali Heydari, Craig A. Thompson, Asif Mehmood* "SoftAdapt: Techniques for Adaptive Loss Weighting of Neural Networks with Multi-Part Loss Functions"
