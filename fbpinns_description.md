# 2.3 FB-PINNs
Технология FBPINNs предложена в работе [[1]](#источники) и пытается обойти проблемы, характерные для PINNS, а именно значительное усложнение задачи оптимизации на больших областях и трудности с обучением высокочастотным колебаниям. 
Обе проблемы заметны при моделировании поведения солитона на больших областях, поэтому данная технология будет подробно рассмотрена далее. Суть подхода заключается в том, что большая область разбивается на много маленьких подобластей, в каждой из которых 
производится независимая нормировка обучающих данных и обучается отдельная PINN. Главное отличие этой технологии от похожих(например [[2]](#источники)) в том, что в функцию потерь не добавляется никаких новых слагаемых, 
потенциально способных усложнить задачу оптимизации. Это достигается за счёт того, что выход каждой из PINN домножается на функцию с компактным носителем, которая начинает убывать при выходе за подобласть. Благодаря этому не нужно накладывать никакие условия 
на границах подобластей: в местах пересечения носителей каждая PINN обучается с учётом значений, которые выдала соседняя с ней PINN. В итоге сумма выходов всех PINN и будет решением, ведь в каждой точке она будет возвращать значение только той PINN, которая 
училась в соответствующей подобласти. Отдельно нужно упомянуть про планировщик, который определяет когда какие PINN должны учиться. В двумерных задачах удобно использовать следующую схему: начать обучение PINN на областях, прилежащих к начальному условию, и 
сдвигать ряд обучаемых PINN каждые n итераций, фиксируя PINN которые уже обучены. Это позволяет избежать ситуаций, когда полученные вдалеке от начального условия решения мешают нахождению решений, удовлетворяющих начальному условию.  
Теперь можно обсудить гиперпараметры, используемые данной технологией:  
* Разбиение на подобласти `x_domains` и `t_domains` определяет насколько маленькие задачи в итоге будут решаться. Авторы [[1]](#источники) указали, что с увеличением числа областей эффективность FBPINNs возрастает.
* Взаимное пересечение областей `intersection` определяет насколько сильно носитель функции выходит за рассматриваемую область. Практика показала, что с увеличением взаимного пересечения областей точность значительно возрастает, но рост вычислительной сложности не позволяет
сделать этот параметр слишком большим и оптимальным значением будет `intersection=2.0`.
* Денормализация `unnorm` - значение, на которое домножаются выходы сети. Эта величина должна быть такой, чтобы нейросети пришлось принимать значения в диапазоне [-1,1]. В рассматриваемой задаче действительная и мнимая части лежат в области [-0.25, 0.25], поэтому логично
принять `unnorm=0.25`.
* Топология нейросети должна соответствовать разбиению на подобласти, а именно уменьшаться с ростом разбиения так, чтобы число обучаемых параметров не было слишком большим для данной задачи.
* Функция активации используется sin, так как показала наибольшую эффективность в сравнении с остальными.
* Разрешение сетки с точками коллокации не должно быть слишком большим, так как это существенно замедляет процесс обучения без значительного прироста к точности.
* Число шагов `steps` при обучении должно быть достаточным для того, чтобы в процсессе обучения достаточно хорошо определить обучаемые параметры.


Согласно провёдённым экспериментам, увеличение `x_domains` приводит к кратному падению точности, в то время как увеличение `t_domains` приводит к заметному увеличению точности при условии сохранения числа обучаемых параметров. То есть вместе с уменьшением размера областей
следует сокращать топологию сети. Наиболее успешным является результат со следующими параметрами: `x_domains=`, `t_domains=`, топология сетей: , `steps=`.  
Статистику по всем проведённым экспериментам можно найти в [таблице](https://github.com/mikhakuv/PINNs/blob/main/statistics/performance_table_fbpinns_description.xlsx)  

# Источники
[1] *B. Moseley, A. Markham, and T. Nissen-Meye* "Finite Basis Physics-Informed Neural Networks(FBPINNs): a scalable domain decomposition approach for solving differential equations."  
[2] *A. D. Jagtap, E. Kharazmi, and G. E. Karniadakis* "Conservative physics-informed neural networks on discrete domains for conservation laws: Applications to forward and inverse problems."  
