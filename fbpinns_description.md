# 2.3 FB-PINNs
Технология FBPINNs предложена в работе [[1]](#источники) и пытается обойти проблемы, характерные для PINNS, а именно значительное усложнение задачи оптимизации на больших областях и трудности с обучением высокочастотным колебаниям. Обе проблемы заметны при моделировании поведения солитона на больших областях, поэтому данная технология будет подробно рассмотрена далее. Суть подхода заключается в том, что большая область разбивается на много маленьких подобластей, в каждой из которых производится независимая нормировка обучающих данных и обучается отдельная PINN. Главное отличие этой технологии от похожих(например [[2]](#источники)) в том, что в функцию потерь не добавляется никаких новых слагаемых, потенциально способных усложнить задачу оптимизации. Это достигается за счёт того, что выход каждой из PINN домножается на функцию с компактным носителем, которая начинает убывать при выходе за подобласть. Благодаря этому не нужно накладывать никакие условия на границах подобластей: в местах пересечения носителей каждая PINN обучается с учётом значений, которые выдала соседняя с ней PINN. В итоге сумма выходов всех PINN и будет решением, ведь в каждой точке она будет возвращать значение только той PINN, которая училась в соответствующей подобласти. Отдельно нужно упомянуть про планировщик, который определяет когда какие PINN должны учиться. В двумерных задачах удобно использовать следующую схему: начать обучение PINN на областях, прилежащих к начальному условию и сдвигать ряд обучаемых PINN каждые n итераций, фиксируя PINN которые уже обучены. Это позволяет избежать ситуаций, когда полученные вдалеке от начального условия решения мешают нахождению решений, удовлетворяющих этому начальному условию.  
Теперь можно обсудить гиперпараметры данной технологии:  
* Разбиение на подобласти `x_domains` и `t_domains` определяет насколько маленькие задачи в итоге будут решаться. Авторы [[1]](#источники) указывают, что с увеличением числа областей эффективность FBPINNs возрастает и это было подтверждено на практике: увеличение `t_domains` приводило к заметному увеличению точности. Тем не менее, данный эффект неэкстраполируем: использование слишком малых областей приводит к падению точности и возникновению артефактов на некоторых областях.  
* Взаимное пересечение областей `intersection` определяет насколько сильно носитель функции выходит за рассматриваемую область. Практика показала, что с увеличением взаимного пересечения областей точность значительно возрастает, но рост вычислительной сложности не позволяет сделать этот параметр слишком большим и оптимальным значением будет `intersection=2.0`.
* Денормализация `unnorm` - значение, на которое домножаются выходы сети. Эта величина должна быть такой, чтобы нейросети пришлось принимать значения в диапазоне [-1,1]. В рассматриваемой задаче действительная и мнимая части лежат в области [-0.5, 0.5], поэтому логично принять `unnorm=0.5`.
* Топология нейросети должна соответствовать разбиению на подобласти, а именно уменьшаться с ростом разбиения так, чтобы число обучаемых параметров не было слишком большим для данной задачи.
* Функция активации используется sin, так как показала наибольшую эффективность в сравнении с остальными.
* Разрешение сетки с точками коллокации не должно быть слишком большим, так как это существенно замедляет процесс обучения без значительного прироста к точности. Во всех опытах использовалась сетка `x_parts=1000`, `t_parts=100`.
* Число шагов `steps` при обучении должно быть достаточным для того чтобы в процессе обучения можно было хорошо определить обучаемые параметры, но при этом не слишком затянуть обученние. Было использовано значение `steps`= $5\cdot 10^{6}$  


Наиболее успешными являются результаты со следующими параметрами:  
* `x_domains`=40, `t_domains`=10, топология сетей: (2,8,8,2), то есть каждая сеть обучается на области $2.5\times 1$ (без учёта пересечения с другой областью), а число обучаемых параметров составляет 45600.

<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/fbpinns_results_7_1.png">
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/fbpinns_results_7_2.png">

(подробнее см exp7 в таблице)
* `x_domains`=40, `t_domains`=20, топология сетей: (2,8,8,2), то есть каждая сеть обучается на области $2.5\times \frac{1}{2}$ (без учёта пересечения с другой областью), а число обучаемых параметров составляет 91200.  Метрики в этом опыте хорошие, но на графиках видны характерные артефакты, которые возникают при слишком мелком разбиении на области.

<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/fbpinns_results_9_1.png">
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/fbpinns_results_9_2.png">

(подробнее см exp9 в таблице).  


Статистику по всем проведённым экспериментам можно найти в [таблице](https://github.com/mikhakuv/PINNs/blob/main/statistics/performance_table_fbpinns_description.xlsx)  

# Источники
[1] *B. Moseley, A. Markham, and T. Nissen-Meye* "Finite Basis Physics-Informed Neural Networks(FBPINNs): a scalable domain decomposition approach for solving differential equations."  
[2] *A. D. Jagtap, E. Kharazmi, and G. E. Karniadakis* "Conservative physics-informed neural networks on discrete domains for conservation laws: Applications to forward and inverse problems."  
