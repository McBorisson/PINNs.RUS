# 2.3 FB-PINNs
Технология FBPINNs предложена в работе [[1]](#источники) и пытается обойти проблемы, характерные для PINNS, а именно значительное усложнение задачи оптимизации на больших областях и трудности с обучением высокочастотным колебаниям. Обе проблемы заметны при моделировании поведения солитона на больших областях, поэтому данная технология будет подробно рассмотрена далее. Суть подхода заключается в том, что большая область разбивается на много маленьких подобластей, в каждой из которых производится независимая нормировка обучающих данных и обучается отдельная PINN. Главное отличие этой технологии от похожих(например [[2]](#источники)) в том, что в функцию потерь не добавляется никаких новых слагаемых, потенциально способных усложнить задачу оптимизации. Это достигается за счёт того, что выход каждой из PINN домножается на функцию с компактным носителем, которая начинает убывать при выходе за подобласть. Благодаря этому не нужно накладывать никакие условия на границах подобластей: в местах пересечения носителей каждая PINN обучается с учётом значений, которые выдала соседняя с ней PINN. В итоге сумма выходов всех PINN и будет решением, ведь в каждой точке она будет возвращать значение только той PINN, которая училась в соответствующей подобласти. Отдельно нужно упомянуть про планировщик, который определяет когда какие PINN должны учиться. В двумерных задачах удобно использовать следующую схему: начать обучение PINN на областях, прилежащих к начальному условию и сдвигать ряд обучаемых PINN каждые n итераций, фиксируя PINN которые уже обучены. Это позволяет избежать ситуаций, когда полученные вдалеке от начального условия решения мешают нахождению решений, удовлетворяющих этому начальному условию.  
Теперь можно обсудить гиперпараметры данной технологии:  
* Разбиение на подобласти `x_domains` и `t_domains` определяет насколько маленькие задачи в итоге будут решаться. Авторы [[1]](#источники) указывают, что с увеличением числа областей эффективность FBPINNs возрастает.
* Взаимное пересечение областей `intersection` определяет насколько сильно носитель функции выходит за рассматриваемую область. Практика показала, что с увеличением взаимного пересечения областей точность значительно возрастает, но рост вычислительной сложности не позволяет сделать этот параметр слишком большим и оптимальным значением будет `intersection=2.0`.
* Денормализация `unnorm` - значение, на которое домножаются выходы сети. Эта величина должна быть такой, чтобы нейросети пришлось принимать значения в диапазоне [-1,1]. В рассматриваемой задаче действительная и мнимая части лежат в области [-0.25, 0.25], поэтому логично принять `unnorm=0.25`.
* Топология нейросети должна соответствовать разбиению на подобласти, а именно уменьшаться с ростом разбиения так, чтобы число обучаемых параметров не было слишком большим для данной задачи.
* Функция активации используется sin, так как показала наибольшую эффективность в сравнении с остальными.
* Разрешение сетки с точками коллокации не должно быть слишком большим, так как это существенно замедляет процесс обучения без значительного прироста к точности. Во всех опытах использовалась сетка `x_parts=1000`, `t_parts=100`.
* Число шагов `steps` при обучении должно быть достаточным для того чтобы в процессе обучения можно было хорошо определить обучаемые параметры.


Согласно провёдённым экспериментам, увеличение `x_domains` приводит к кратному падению точности, в то время как увеличение `t_domains` приводит к заметному увеличению точности при условии сохранения числа обучаемых параметров. То есть вместе с уменьшением размера областей следует сокращать топологию сетей, чтобы число обучаемых параметров всей сети не увеличивалось. Важно отметить, что данный эффект неэкстраполируем: если брать слишком малые области и сети с очень малым числом параметров, то точность начнёт резко падать.  
Наиболее успешными являются результаты со следующими параметрами:  
* `x_domains`=40, `t_domains`=10, топология сетей: (2,8,8,2), то есть каждая сеть обучается на области $2.5\times 1$ без учёта пересечения с другой областью, а число обучаемых параметров составляет 45600
(подробнее см exp_fa_2 и exp_fa_2+ в таблице).
* `x_domains`=40, `t_domains`=30, топология сетей: (2,4,4,2), то есть каждая сеть обучается на области $2.5\times \frac{1}{3}$ без учёта пересечения с другой областью, а число обучаемых параметров составляет 50400
(подробнее см exp_fa_7 и exp_fa_7+ в таблице).


Статистику по всем проведённым экспериментам можно найти в [таблице](https://github.com/mikhakuv/PINNs/blob/main/statistics/performance_table_fbpinns_description.xlsx)  

# Источники
[1] *B. Moseley, A. Markham, and T. Nissen-Meye* "Finite Basis Physics-Informed Neural Networks(FBPINNs): a scalable domain decomposition approach for solving differential equations."  
[2] *A. D. Jagtap, E. Kharazmi, and G. E. Karniadakis* "Conservative physics-informed neural networks on discrete domains for conservation laws: Applications to forward and inverse problems."  
