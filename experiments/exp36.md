В данном эксперименте проводятся опыты с линейными заменами, которые по идее должны облегчить аппроксимацию искомой функции. Пояснение дано ниже.  

Мы хотим аппроксимировать такую функцию:  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/exp36_capture_1.png">  
Но она слишком резкая, из-за чего ошибка при обучении высокая. Поэтому сделаем замену переменной, растянув график по оси ординат:  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/exp36_capture_2.png">  
Именно на таких данных и обучаем модель. А когда она обучится, сделаем обратную замену и получится график:  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/exp36_capture_3.png">  

Есть и другая замена, уменьшающая амплитуду колебаний. Она заключается в том, чтобы вместо исходной функции:  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/exp36_capture_4.png">  
Аппроксимировать менее резкую функцию:  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/exp36_capture_5.png">  
И после того, как нейросеть обучится на таких данных, просто увеличиваем амплитуду её выхода и получаем исходную функцию.  

Результаты обоих способов получились плохие и вот почему: Функция, задающая начальное условие это не какая-то случайная функция, а одно из возможных аналитических решений уравнения, которое приводит к возникновению солитона.
И если мы её меняем(а мы её как раз меняем: увеличиваем амплитуду или уменьшаем частоту относительно x), то она перестаёт быть тем решением, которое задаёт солитон и происходит распыление солитона. А когда солитон распылён,
увеличение амплитуды или частоты уже не поможет и получатся такие картинки:  
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/exp36_capture_6.png">(это второй способ)   
<img src="https://github.com/mikhakuv/PINNs/blob/main/pictures/exp36_capture_7.png">(это первый способ)   
