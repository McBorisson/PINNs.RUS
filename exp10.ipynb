{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sgqfu6cegA2T"
      },
      "source": [
        "Реализация PINN на pytorch взята отсюда: https://github.com/jayroxis/PINNs/blob/master/Burgers%20Equation/Burgers%20Inference%20(PyTorch).ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSj6OaovBhEG"
      },
      "source": [
        "#Уравнение второго порядка:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md2b5DS8oB47"
      },
      "source": [
        "коэффициенты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "h9ilHPpTRKTs"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "n=1\n",
        "a=1.\n",
        "alpha=1.\n",
        "beta=1.\n",
        "k=1.\n",
        "z0=-55.\n",
        "th0=0.\n",
        "w=9/8\n",
        "l = (4. * alpha * n ** 2) / (a + n * a)\n",
        "nu = (4. * beta * n ** 2) / (a + 2. * n * a)\n",
        "mu = (4. * n ** 2 * (w - a * k ** 2)) / a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIrdeugOoGEJ"
      },
      "source": [
        "решение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3ULNwsGHQ4Fa"
      },
      "outputs": [],
      "source": [
        "def q(x,t):\n",
        "  const = math.exp((x-2*a*k*t-z0)*math.sqrt(mu))\n",
        "  return [math.pow(((4*mu) / ((1/const)+2*l+(l**2-4*mu*nu)*const)), 1/(2*n)) * math.cos(k*x-w*t+th0),\n",
        "  math.pow(((4*mu) / ((1/const)+2*l+(l**2-4*mu*nu)*const)), 1/(2*n)) * math.sin(k*x-w*t+th0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6mPmy7FW508",
        "outputId": "48c90a53-da4a-4201-b66a-e9308e393e4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyDOE\n",
            "  Downloading pyDOE-0.3.8.zip (22 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyDOE) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyDOE) (1.10.1)\n",
            "Building wheels for collected packages: pyDOE\n",
            "  Building wheel for pyDOE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyDOE: filename=pyDOE-0.3.8-py3-none-any.whl size=18169 sha256=abca2b9b238b62a30611125870aaab3a76ccbae621a481665cb18e163e337a9d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/b6/d7/c6b64746dba6433c593e471e0ac3acf4f36040456d1d160d17\n",
            "Successfully built pyDOE\n",
            "Installing collected packages: pyDOE\n",
            "Successfully installed pyDOE-0.3.8\n"
          ]
        }
      ],
      "source": [
        "!pip install pyDOE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ESHqRF_KBd1A"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from collections import OrderedDict #упорядоченный словарь\n",
        "from pyDOE import lhs #функция, выбирающая значения для обучения на них нейросети\n",
        "import numpy as np\n",
        "import time\n",
        "np.random.seed(1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "I-K7dJndUlAB"
      },
      "outputs": [],
      "source": [
        "#можно сменить среду выполнения на gpu и обучение будет происходить быстрее\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def q_tensor(x,t): #тензорная функция q, принимающая на вход тензоры из координат x и t и возвращающая тензоры из u и v\n",
        "  const = torch.exp((x-2*a*k*t-z0)*math.sqrt(mu))\n",
        "  return [torch.pow(((4*mu*const) / (1+2*l*const+(l**2-4*mu*nu)*const**2)), 1/(2*n)) * torch.cos(k*x-w*t+th0),\n",
        "                             torch.pow(((4*mu*const) / (1+2*l*const+(l**2-4*mu*nu)*const**2)), 1/(2*n))*torch.sin(k*x-w*t+th0)]"
      ],
      "metadata": {
        "id": "D_JvKh-eUGN0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SinActivation(torch.nn.Module): #кастомная функция активации - sin\n",
        "    def __init__(self):\n",
        "        super(SinActivation, self).__init__()\n",
        "        return\n",
        "    def forward(self, x):\n",
        "        return torch.sin(x)"
      ],
      "metadata": {
        "id": "Lgsje_Ms8Vmr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INnh3lfZpdTk"
      },
      "source": [
        "нейросеть, в виде которой будет находиться решение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iZq8rBGZUqMc"
      },
      "outputs": [],
      "source": [
        "class DNN(torch.nn.Module):\n",
        "    def __init__(self, layers): #принимает на вход массив целых чисел\n",
        "        super(DNN, self).__init__() #вызывает метод init(почему нельзя сделать это без super?)\n",
        "\n",
        "        self.depth = len(layers) - 1\n",
        "        self.activation = SinActivation #в качестве функции активации используется tanh\n",
        "\n",
        "        layer_list = list() #список с весами и функциями активации для каждого слоя\n",
        "        for i in range(self.depth - 1):\n",
        "            layer_list.append(\n",
        "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1])) #каждые два слоя образуют двудольный граф\n",
        "            )\n",
        "            layer_list.append(('activation_%d' % i, self.activation()))\n",
        "\n",
        "        layer_list.append(\n",
        "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1])) #нельзя сделать в цикле, потому что нет функции активации\n",
        "        )\n",
        "\n",
        "        layerDict = OrderedDict(layer_list) #сделали упорядоченный словарь, чтобы при использовании элементы выдавались в том порядке, в котором были добавлены\n",
        "\n",
        "        # deploy layers\n",
        "        self.layers = torch.nn.Sequential(layerDict) #собственно, задали архитектуру нейросети\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layers(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYh7EE5hpjkO"
      },
      "source": [
        "наконец, PINN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DqcQWmI3Uuy_"
      },
      "outputs": [],
      "source": [
        "class PhysicsInformedNN():\n",
        "    def __init__(self, X_uv, u, v, X_f, layers, lb, ub, a, alpha, beta):\n",
        "\n",
        "        # граничные условия\n",
        "        self.lb = torch.tensor(lb).float().to(device) #левое\n",
        "        self.ub = torch.tensor(ub).float().to(device) #правое\n",
        "\n",
        "        # данные для обучения\n",
        "        self.x_uv = torch.tensor(X_uv[:, 0:1], requires_grad=True).float().to(device) #для граничных условий: (x_uv, t_uv, u, v)\n",
        "        self.t_uv = torch.tensor(X_uv[:, 1:2], requires_grad=True).float().to(device)\n",
        "        self.u = torch.tensor(u).float().to(device)\n",
        "        self.v = torch.tensor(v).float().to(device)\n",
        "        #self.x_f = torch.tensor(X_f[:, 0:1], requires_grad=True).float().to(device) #для уравнения: (x_f, t_f, f=0) (используются переданные сети точки)\n",
        "        #self.t_f = torch.tensor(X_f[:, 1:2], requires_grad=True).float().to(device)\n",
        "        self.x_f = torch.tensor((lb + (ub-lb)*lhs(2, N_f))[:, 0:1], requires_grad=True).float().to(device) #создаём N_f точек для обучения соответствия уравнению. те, что переданы, не используем\n",
        "        self.t_f = torch.tensor((lb + (ub-lb)*lhs(2, N_f))[:, 1:2], requires_grad=True).float().to(device)\n",
        "\n",
        "        #данные для законов сохранения\n",
        "        self.x_fragments = 100 #разбиение при интегрировании по x при фиксированном t\n",
        "        self.t_amount = 10 #в скольких точках t считается интеграл\n",
        "        x_l = np.linspace(x_0, x_1, self.x_fragments).reshape(self.x_fragments,1)\n",
        "        t_l = np.linspace(t_0, t_1, self.t_amount).reshape(self.t_amount,1)\n",
        "        self.x_l = torch.tensor(x_l, requires_grad=True).float().to(device)\n",
        "        self.t_l = torch.tensor(t_l, requires_grad=True).float().to(device)\n",
        "\n",
        "        # числовые коэффициенты в уравнении\n",
        "        self.a = a\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "\n",
        "        # модель\n",
        "        self.layers = layers\n",
        "        self.dnn = DNN(layers).to(device)\n",
        "\n",
        "        # loss, которые нужны для коэффициентов в балансировке весов\n",
        "        u_pred, v_pred = self.net_uv(self.x_uv, self.t_uv)\n",
        "        f_u_pred, f_v_pred = self.net_f(self.x_f, self.t_f)\n",
        "        self.L_uv = torch.mean(((self.u - u_pred) ** 2 + (self.v - v_pred) ** 2)/2) #средний квадрат всех отклонений от начальных условий\n",
        "        self.L_f = torch.mean((f_u_pred ** 2 + f_v_pred ** 2)/2) #средний квадрат всех отклонений от условия\n",
        "\n",
        "        # оптимизатор - LBFGS, обучает с точностью до 1e-5 или пока разница в точности уменьшается больше, чем точность float(?)\n",
        "        self.optimizer = torch.optim.LBFGS(\n",
        "            self.dnn.parameters(),\n",
        "            lr=0.0001,\n",
        "            max_iter=50000,\n",
        "            max_eval=50000,\n",
        "            history_size=50,\n",
        "            tolerance_grad=1e-5,\n",
        "            tolerance_change=1.0 * np.finfo(float).eps,\n",
        "            #line_search_fn=\"strong_wolfe\"\n",
        "        )\n",
        "\n",
        "        self.adam = torch.optim.Adam(\n",
        "          self.dnn.parameters(),\n",
        "          lr=0.005,\n",
        "          betas=(0.9, 0.999),\n",
        "          eps=1e-08,\n",
        "          weight_decay=0,\n",
        "          amsgrad=False,\n",
        "          foreach=None,\n",
        "          maximize=False,\n",
        "          capturable=False,\n",
        "          differentiable=False,\n",
        "          fused=None)\n",
        "\n",
        "        self.iter = 0\n",
        "\n",
        "    def net_uv(self, x, t): # вывод модели\n",
        "        u = self.dnn(torch.cat([x, t], dim=1))[:,0:1]\n",
        "        v = self.dnn(torch.cat([x, t], dim=1))[:,1:2]\n",
        "        return u, v\n",
        "\n",
        "    def net_f(self, x, t): #вывод  функции\n",
        "        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n",
        "        u, v = self.net_uv(x, t)\n",
        "\n",
        "        u_t = torch.autograd.grad(\n",
        "            u, t,\n",
        "            grad_outputs=torch.ones_like(u),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0] #производая по t\n",
        "        u_x = torch.autograd.grad(\n",
        "            u, x,\n",
        "            grad_outputs=torch.ones_like(u),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0] #производная по x\n",
        "        u_xx = torch.autograd.grad(\n",
        "            u_x, x,\n",
        "            grad_outputs=torch.ones_like(u_x),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0] #вторая призводная по x\n",
        "\n",
        "        v_t = torch.autograd.grad(\n",
        "            v, t,\n",
        "            grad_outputs=torch.ones_like(v),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0] #производая по t\n",
        "        v_x = torch.autograd.grad(\n",
        "            v, x,\n",
        "            grad_outputs=torch.ones_like(v),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0] #производная по x\n",
        "        v_xx = torch.autograd.grad(\n",
        "            v_x, x,\n",
        "            grad_outputs=torch.ones_like(v_x),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0] #вторая призводная по x\n",
        "\n",
        "        f_u = -v_t + (self.a*u_xx + self.alpha*u*(u**2+v**2) - self.beta*u*(u**2+v**2)**2) #это и есть действительная и коплексная части исходного уравнения\n",
        "        f_v = u_t + (self.a*v_xx + self.alpha*v*(u**2+v**2) - self.beta*v*(u**2+v**2)**2) #они получены путём подстановки q=u+i*v в него\n",
        "        return f_u, f_v\n",
        "\n",
        "    def first_law(self): #реализация первого закона сохранения\n",
        "        x_step=(self.x_l[1]-self.x_l[0]).item()\n",
        "        initial_integral=0\n",
        "        for j in range(0,self.x_fragments): #считаем интеграл по x в точке t_0\n",
        "          u, v = self.net_uv(self.x_l[j:j+1], self.t_l[0:1]) #интеграл считается от вывода нейросети\n",
        "          #u, v = q_tensor(self.x_l[j:j+1], self.t_l[0:1]) #интеграл считается от q(x,t), ведь она по сути дана в t_0\n",
        "          initial_integral += (u**2 + v**2) * x_step\n",
        "\n",
        "        variation=np.zeros(self.t_amount)\n",
        "        for i in range(1,self.t_amount): #сравниваем с интегралом по x в других точках\n",
        "          integral=0\n",
        "          for j in range(0,self.x_fragments):\n",
        "            u, v = self.net_uv(self.x_l[j:j+1], self.t_l[i:i+1])\n",
        "            integral += (u**2 + v**2) * x_step\n",
        "          variation[i]=(initial_integral-integral)**2\n",
        "\n",
        "        return variation.mean(axis=0) #возвращаем средний квадрат отклонения\n",
        "\n",
        "    def second_law(self): #реализация второго закона сохранения\n",
        "        x_step=(self.x_l[1]-self.x_l[0]).item()\n",
        "        initial_integral=0\n",
        "        for j in range(0,self.x_fragments): #считаем интеграл по x в t_0\n",
        "          u, v = self.net_uv(self.x_l[j:j+1], self.t_l[0:1]) #интеграл считается от вывода нейросети\n",
        "          #u, v = q_tensor(self.x_l[j:j+1], self.t_l[0:1]) #интеграл считается от q(x,t)\n",
        "          u_x = torch.autograd.grad(u, self.x_l, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0]\n",
        "          v_x = torch.autograd.grad(v, self.x_l, grad_outputs=torch.ones_like(v), retain_graph=True, create_graph=True)[0]\n",
        "          initial_integral += ((v_x[j] * u - u_x[j] * v) * x_step)\n",
        "\n",
        "        variation=np.zeros(self.t_amount)\n",
        "        for i in range(1,self.t_amount): #сравниваем с интегралом по x в других точках\n",
        "          integral=0\n",
        "          for j in range(0,self.x_fragments):\n",
        "            u, v = self.net_uv(self.x_l[j:j+1], self.t_l[i:i+1])\n",
        "            u_x = torch.autograd.grad(u, self.x_l, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0] #почему-то градиент по self.x_l[j:j+1] посчитать не получается\n",
        "            v_x = torch.autograd.grad(v, self.x_l, grad_outputs=torch.ones_like(v), retain_graph=True, create_graph=True)[0] #поэтому вычисляется градиент по self.x_l и берётся j элемент\n",
        "            integral += ((v_x[j] * u - u_x[j] * v) * x_step)\n",
        "          variation[i]=(initial_integral-integral)**2\n",
        "\n",
        "        return variation.mean(axis=0) #опять возвращаем средний квадрат отклонения\n",
        "\n",
        "    def loss_func(self): #функция потерь\n",
        "        self.optimizer.zero_grad() #обнуляет градиенты\n",
        "\n",
        "        u_pred, v_pred = self.net_uv(self.x_uv, self.t_uv)\n",
        "        f_u_pred, f_v_pred = self.net_f(self.x_f, self.t_f)\n",
        "        loss_uv = torch.mean(((self.u - u_pred) ** 2 + (self.v - v_pred) ** 2)/2) #средний квадрат всех отклонений от начальных условий\n",
        "        loss_f = torch.mean((f_u_pred ** 2 + f_v_pred ** 2)/2) #средний квадрат всех отклонений от условия\n",
        "\n",
        "        L_uv_prev = self.L_uv #реализация метода балансировки коэффициентов Soft Adapt\n",
        "        L_f_prev = self.L_f\n",
        "        self.L_uv = loss_uv\n",
        "        self.L_f = loss_f\n",
        "        lambda_uv=math.exp(self.L_uv/L_uv_prev)/(math.exp(self.L_uv/L_uv_prev) + math.exp(self.L_f/L_f_prev))\n",
        "        lambda_f=math.exp(self.L_f/L_f_prev)/(math.exp(self.L_uv/L_uv_prev) + math.exp(self.L_f/L_f_prev))\n",
        "\n",
        "        lambda_uv_array.append(lambda_uv) #записываем коэффициенты, чтобы потом построить график\n",
        "        lambda_f_array.append(lambda_f)\n",
        "        iter_s_array.append(self.iter)\n",
        "\n",
        "        #loss_fl = self.first_law()\n",
        "        #loss_sl = self.second_law()\n",
        "\n",
        "        #loss = loss_uv + loss_f + loss_fl + loss_sl\n",
        "        loss = lambda_uv*loss_uv + lambda_f*loss_f\n",
        "\n",
        "        loss.backward()\n",
        "        self.iter += 1\n",
        "        if self.iter % 100 == 0:\n",
        "            print(\n",
        "                'Iter %d, Loss: %.5e, Loss_uv: %.5e, Loss_f: %.5e' % (self.iter, loss.item(), loss_uv.item(), loss_f.item())\n",
        "            )\n",
        "            #print('loss_fl: %.5e' %loss_fl.item())\n",
        "            #print('loss_sl: %.5e' %loss_sl.item())\n",
        "            u_pred_current, v_pred_current, f_u_pred_current, f_v_pred_current = self.predict(X_star)\n",
        "            mse_q_current = (((u_star**2+v_star**2)**0.5 - (u_pred_current**2+v_pred_current**2)**0.5)**2).mean(axis=0).item() #средний квадрат разности модулей\n",
        "            mse_q_array.append(mse_q_current) #запоминаем mse_q в текущей стадии обучения чтобы потом построить график\n",
        "            loss_array.append(loss.item()) #loss тоже запоминаем для графика\n",
        "            iter_array.append(self.iter)\n",
        "\n",
        "        if self.iter % 10 == 0: #каждые 10 итераций генерируем новые точки для обучения\n",
        "          random_points = lb + (ub-lb)*lhs(2, N_f)\n",
        "          self.x_f = torch.tensor(random_points[:, 0:1], requires_grad=True).float().to(device)\n",
        "          self.t_f = torch.tensor(random_points[:, 1:2], requires_grad=True).float().to(device)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def train(self): #обучение\n",
        "        print('training started')\n",
        "        print('100000 iterations of ADAM:')\n",
        "        self.dnn.train()\n",
        "        for i in range(100000): #во время тренировки производится 100000 шагов adam, а дальше запускается lbfgs\n",
        "            if i % 100 == 0: self.adam.param_groups[0]['lr'] = 0.997*self.adam.param_groups[0]['lr'] #экспоненциальное уменьшение шага каждые 100 шагов\n",
        "            self.adam.step(self.loss_func)\n",
        "        print('LBFGS:')\n",
        "        self.optimizer.step(self.loss_func)\n",
        "\n",
        "\n",
        "    def predict(self, X): #вывод нейросети и функции на входных данных\n",
        "        x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n",
        "        t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n",
        "\n",
        "        self.dnn.eval()\n",
        "        u, v = self.net_uv(x, t)\n",
        "        f_u, f_v = self.net_f(x, t)\n",
        "        u = u.detach().cpu().numpy()\n",
        "        v = v.detach().cpu().numpy()\n",
        "        f_u = f_u.detach().cpu().numpy()\n",
        "        f_v = f_v.detach().cpu().numpy()\n",
        "        return u, v, f_u, f_v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFMEDI4_pqZL"
      },
      "source": [
        "данные для обучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XdohH47tU0cf"
      },
      "outputs": [],
      "source": [
        "N_u = 200 # число точек, обучающих принимать граничные значения\n",
        "N_f = 30000 # число точек, обучающих удовлетворять уравнению\n",
        "x_parts = 1000 #число частей, на которые разбивается отрезок x\n",
        "t_parts = 100 #число частей, на которые разбивается отрезок t\n",
        "layers = [2, 100, 100, 100, 100, 2] #2 входа, 2 выхода и 4 слоя по 100 нейронов\n",
        "\n",
        "x_0=-85\n",
        "x_1=85\n",
        "t_0=0\n",
        "t_1=50\n",
        "x = np.linspace(x_0, x_1, x_parts)\n",
        "t = np.linspace(t_0, t_1, t_parts)\n",
        "X, T = np.meshgrid(x, t)\n",
        "Exact_u=np.zeros((t_parts,x_parts))\n",
        "Exact_v=np.zeros((t_parts,x_parts))\n",
        "for i in range(0,t_parts):\n",
        "  for j in range(0,x_parts):\n",
        "    Exact_u[i][j]=q(X[i][j],T[i][j])[0]\n",
        "    Exact_v[i][j]=q(X[i][j],T[i][j])[1]\n",
        "\n",
        "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
        "u_star = Exact_u.flatten()[:,None]\n",
        "v_star = Exact_v.flatten()[:,None]\n",
        "\n",
        "# границы области\n",
        "lb = X_star.min(0)\n",
        "ub = X_star.max(0)\n",
        "\n",
        "# для обучения берём только данные на границах области(граничные условия по сути)\n",
        "xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T)) #(x,t_0)\n",
        "uu1 = Exact_u[0:1,:].T\n",
        "vv1 = Exact_v[0:1,:].T\n",
        "xx2 = np.hstack((X[:,0:1], T[:,0:1])) #(x_0,t)\n",
        "uu2 = np.zeros((t_parts,1)) #граничные условия поставил 0 в соответствии с требованиями\n",
        "vv2 = np.zeros((t_parts,1))\n",
        "xx3 = np.hstack((X[:,-1:], T[:,-1:])) #(x_1,t)\n",
        "uu3 = np.zeros((t_parts,1)) #тут тоже\n",
        "vv3 = np.zeros((t_parts,1))\n",
        "\n",
        "X_uv_train = np.vstack([xx1, xx2, xx3]) #данные для тренировки в точках на границе\n",
        "u_train = np.vstack([uu1, uu2, uu3])\n",
        "v_train = np.vstack([vv1, vv2, vv3])\n",
        "X_f_train = lb + (ub-lb)*lhs(2, N_f) #данные для тренировки в случайных точкам из области\n",
        "X_f_train = np.vstack((X_f_train, X_uv_train)) #добавим к ним ещё и точки на границе, там f тоже 0\n",
        "\n",
        "idx = np.random.choice(X_uv_train.shape[0], N_u, replace=False) #выберем из точек на границе только N_u\n",
        "X_uv_train = X_uv_train[idx, :]\n",
        "u_train = u_train[idx,:]\n",
        "v_train = v_train[idx,:]\n",
        "\n",
        "mse_q_array = [] #массив с mse_q для каждой сотой итерации обучения\n",
        "loss_array = [] #аналогичный массив с loss\n",
        "iter_array = [] #итерации с шагом 100\n",
        "lambda_uv_array = [] #массивы с коэффициентами на каждой итерации обучения\n",
        "lambda_f_array = []\n",
        "iter_s_array = [] #итерации с шагом 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwUjv7emVVqP"
      },
      "source": [
        "Обучение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "McQX7sQ5U9DB"
      },
      "outputs": [],
      "source": [
        "model = PhysicsInformedNN(X_uv_train, u_train, v_train, X_f_train, layers, lb, ub, a, alpha, beta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zAUFaIJVGZd"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "#штука сверху выведет итоговое время выполнения\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(np.array(iter_array), np.array(loss_array), color = \"blue\")\n",
        "#plt.ylim(0,5e-5)\n",
        "plt.title('loss(iter)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pZrbXMDJHAPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.array(iter_array), np.array(mse_q_array), color = \"blue\")\n",
        "plt.title('mse_q(iter)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KP6Mcf2VnITC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.array(iter_s_array), np.array(lambda_uv_array), color = \"orange\")\n",
        "plt.plot(np.array(iter_s_array), np.array(lambda_f_array), color = \"red\")\n",
        "#plt.plot(np.array(iter_s_array)[::500], np.array(lambda_uv_array)[::500], color = \"orange\")\n",
        "#plt.plot(np.array(iter_s_array)[::500], np.array(lambda_f_array)[::500], color = \"red\")\n",
        "plt.title('lambda(iter)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kCV6ylMvZq-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd #сохраняем в таблицу статистику обучения\n",
        "df = pd.DataFrame([np.array(mse_q_array), np.array(loss_array), np.array(iter_array)], index=['mse_q', 'loss', 'iter'])\n",
        "df.to_excel(\"stats.xlsx\")"
      ],
      "metadata": {
        "id": "QwY2khYl0MD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOGdVkXCVIlH"
      },
      "outputs": [],
      "source": [
        "u_pred, v_pred, f_u_pred, f_v_pred = model.predict(X_star) #предсказания модели в точках, в которых известно численное решение\n",
        "\n",
        "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2) #относительная точность найденной функции u\n",
        "error_v = np.linalg.norm(v_star-v_pred,2)/np.linalg.norm(v_star,2) # и то же самое для v\n",
        "error_f = np.linalg.norm(f_u_pred+f_v_pred,2) #абсолютная точность выполнения условий уравнения f=0\n",
        "print('Rel error u: %e, Rel error v: %e, Abs error f: %e' % (error_u, error_v, error_f)) #довольно неплохие результаты, учитывая, что вектор f_pred имеет длину x_parts*t_parts=100000\n",
        "\n",
        "mse_u = ((u_star-u_pred)**2).mean(axis=0).item()\n",
        "mse_v = ((v_star-v_pred)**2).mean(axis=0).item()\n",
        "mse_q = (((u_star**2+v_star**2)**0.5 - (u_pred**2+v_pred**2)**0.5)**2).mean(axis=0).item() #средний квадрат разности модулей\n",
        "mse_f_u = ((f_u_pred)**2).mean(axis=0).item()\n",
        "mse_f_v = ((f_v_pred)**2).mean(axis=0).item()\n",
        "print('MSE_u: %e, MSE_v: %e, MSE_q: %e, MSE_f_u: %e, MSE_f_v: %e' %(mse_u, mse_v, mse_q, mse_f_u, mse_f_v)) #среднеквадратичное отклонение от известного решения во всех точках сетки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7y79Xiye9UE"
      },
      "source": [
        "Визуализация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FG5HJmUQOmH7"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "u_min, u_max = -np.abs(u_pred).max(), np.abs(u_pred).max()\n",
        "c = ax.pcolormesh(T, X, u_pred.reshape((t_parts, x_parts)), cmap='RdBu', vmin=u_min, vmax=u_max)\n",
        "ax.set_title('u_pred(x,t)')\n",
        "ax.axis([t_0, t_1, x_0, x_1])\n",
        "fig.colorbar(c, ax=ax)\n",
        "#plt.scatter(X_uv_train[:,1], X_uv_train[:,0], s=7, color='black')\n",
        "#plt.scatter(X_f_train[:,1], X_f_train[:,0], s=3, color='black', alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "u_min, u_max = -np.abs(Exact_u).max(), np.abs(Exact_u).max()\n",
        "c = ax.pcolormesh(T, X, Exact_u, cmap='RdBu', vmin=u_min, vmax=u_max)\n",
        "ax.set_title('u_truth(x,t)')\n",
        "ax.axis([t_0, t_1, x_0, x_1])\n",
        "fig.colorbar(c, ax=ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GPMVVPdV-CW"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "v_min, v_max = -np.abs(v_pred).max(), np.abs(v_pred).max()\n",
        "c = ax.pcolormesh(T, X, v_pred.reshape((t_parts, x_parts)), cmap='RdBu', vmin=v_min, vmax=v_max)\n",
        "ax.set_title('v_pred(x,t)')\n",
        "ax.axis([t_0, t_1, x_0, x_1])\n",
        "fig.colorbar(c, ax=ax)\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "v_min, v_max = -np.abs(Exact_v).max(), np.abs(Exact_v).max()\n",
        "c = ax.pcolormesh(T, X, Exact_v, cmap='RdBu', vmin=v_min, vmax=v_max)\n",
        "ax.set_title('v_truth(x,t)')\n",
        "ax.axis([t_0, t_1, x_0, x_1])\n",
        "fig.colorbar(c, ax=ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(t_0,t_1+1, 10):\n",
        "  current_row = int(t_parts*(t-t_0)/(t_1-t_0))\n",
        "  if t==t_1: current_row=-1 #если смотрим значения в t_1, то ясно что это последняя колонка\n",
        "  plt.plot(X[current_row, :], u_pred.reshape((t_parts, x_parts))[current_row,:], color = \"red\") #предсказанное решение\n",
        "  plt.plot(X[current_row, :], Exact_u[current_row,:], color = \"green\") #аналитическое решение\n",
        "  plt.title('u_pred(x,t=%.2f)'%t)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "VJGf_wB1Afx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(t_0,t_1+1, 10):\n",
        "  current_row = int(t_parts*(t-t_0)/(t_1-t_0))\n",
        "  if t==t_1: current_row=-1 #если смотрим значения в t_1, то ясно что это последняя колонка\n",
        "  plt.plot(X[current_row, :], v_pred.reshape((t_parts, x_parts))[current_row,:], color = \"red\")\n",
        "  plt.plot(X[current_row, :], Exact_v[current_row,:], color = \"green\")\n",
        "  plt.title('v_pred(x,t=%.2f)'%t)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "VpfujHIVAmoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "err = ((u_star-u_pred)**2 + (v_star-v_pred)**2)**0.5 #модуль разности q_truth и q_pred\n",
        "fig, ax = plt.subplots()\n",
        "e_min, e_max = err.min(), err.max()\n",
        "c = ax.pcolormesh(T, X, err.reshape((t_parts, x_parts)), cmap='binary', vmin=e_min, vmax=e_max)\n",
        "ax.set_title('|q_truth-q_pred|(x,t)')\n",
        "ax.axis([t_0, t_1, x_0, x_1])\n",
        "fig.colorbar(c, ax=ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R0W6HXlPe7ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "err = (((u_star**2+v_star**2)**0.5 - (u_pred**2+v_pred**2)**0.5)**2)\n",
        "fig, ax = plt.subplots()\n",
        "e_min, e_max = err.min(), err.max()\n",
        "c = ax.pcolormesh(T, X, err.reshape((t_parts, x_parts)), cmap='binary', vmin=e_min, vmax=e_max)\n",
        "ax.set_title('|q_truth|-|q_pred| (x,t)')\n",
        "ax.axis([t_0, t_1, x_0, x_1])\n",
        "fig.colorbar(c, ax=ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U8UNsXFcXcEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверка законов сохранения"
      ],
      "metadata": {
        "id": "A70zjI84sMw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_fragments = 100 #разбиение при интегрировании по x при фиксированном t\n",
        "t_amount = 10 #в скольких точках t считается интеграл\n",
        "x_l = np.linspace(x_0, x_1, x_fragments).reshape(x_fragments,1)\n",
        "t_l = np.linspace(t_0, t_1, t_amount).reshape(t_amount,1)\n",
        "X_l = torch.tensor(x_l, requires_grad=True).float().to(device)\n",
        "T_l = torch.tensor(t_l, requires_grad=True).float().to(device)\n",
        "\n",
        "#реализация первого закона сохранения\n",
        "x_step=(X_l[1]-X_l[0]).item()\n",
        "initial_integral=0\n",
        "for j in range(0,x_fragments): #считаем интеграл по x в точке t_0\n",
        "  u, v = model.net_uv(X_l[j:j+1], T_l[0:1])\n",
        "  initial_integral += (u**2 + v**2) * x_step\n",
        "\n",
        "first_variation=np.zeros(t_amount)\n",
        "for i in range(1,t_amount): #сравниваем с интегралом по x в других точках\n",
        "  integral=0\n",
        "  for j in range(0,x_fragments):\n",
        "    u, v =model.net_uv(X_l[j:j+1], T_l[i:i+1])\n",
        "    integral += (u**2 + v**2) * x_step\n",
        "  first_variation[i]=(initial_integral-integral)**2\n",
        "MSE_fl = first_variation.mean(axis=0).item() #возвращаем средний квадрат отклонения\n",
        "\n",
        "\n",
        "initial_integral=0\n",
        "for j in range(0,x_fragments): #считаем интеграл по x в t_0\n",
        "  u, v = model.net_uv(X_l[j:j+1], T_l[0:1])\n",
        "  u_x = torch.autograd.grad(u, X_l, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0]\n",
        "  v_x = torch.autograd.grad(v, X_l, grad_outputs=torch.ones_like(v), retain_graph=True, create_graph=True)[0]\n",
        "  initial_integral += ((v_x[j] * u - u_x[j] * v) * x_step)\n",
        "\n",
        "second_variation=np.zeros(t_amount)\n",
        "for i in range(1,t_amount): #сравниваем с интегралом по x в других точках\n",
        "  integral=0\n",
        "  for j in range(0,x_fragments):\n",
        "    u, v = model.net_uv(X_l[j:j+1], T_l[i:i+1])\n",
        "    u_x = torch.autograd.grad(u, X_l, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0] #почему-то градиент по self.x_l[j:j+1] посчитать не получается\n",
        "    v_x = torch.autograd.grad(v, X_l, grad_outputs=torch.ones_like(v), retain_graph=True, create_graph=True)[0] #поэтому вычисляется градиент по self.x_l и берётся j элемент\n",
        "    integral += ((v_x[j] * u - u_x[j] * v) * x_step)\n",
        "  second_variation[i]=(initial_integral-integral)**2\n",
        "MSE_sl = second_variation.mean(axis=0).item() #опять возвращаем средний квадрат отклонения\n",
        "\n",
        "\n",
        "print('First law MSE: %e, Second law MSE: %e' %(MSE_fl, MSE_sl))"
      ],
      "metadata": {
        "id": "y42n1oFPsSsp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}