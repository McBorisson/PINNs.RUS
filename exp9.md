В этом эксперименте выбраны другие гиперпараметры, архитектура и использовано два оптимизатора:  
<https://colab.research.google.com/drive/1rANA9yvwnv6syQcZG8Fh3yBVY4RX6HOd?usp=sharing>  
Теперь z0=-55; -85<x<85 0<t<50; архитектура: [2,100,100,100,100,2]; каждые 10 итераций данные для обучения условиям уравнения перегенерируются: оптимизация происходит в два этапа:
сначала 100000 итераций adam с экспоненциальным уменьшением learning rate(lr=0.997*lr каждые 100 итераций) а потом lbfgs
