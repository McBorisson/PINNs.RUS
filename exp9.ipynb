{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sgqfu6cegA2T"
      },
      "source": [
        "Реализация PINN на pytorch взята отсюда: https://github.com/jayroxis/PINNs/blob/master/Burgers%20Equation/Burgers%20Inference%20(PyTorch).ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSj6OaovBhEG"
      },
      "source": [
        "#Уравнение второго порядка:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md2b5DS8oB47"
      },
      "source": [
        "коэффициенты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "h9ilHPpTRKTs"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "n=1\n",
        "a=1.\n",
        "alpha=1.\n",
        "beta=1.\n",
        "k=1.\n",
        "z0=-55.\n",
        "th0=0.\n",
        "w=9/8\n",
        "l = (4. * alpha * n ** 2) / (a + n * a)\n",
        "nu = (4. * beta * n ** 2) / (a + 2. * n * a)\n",
        "mu = (4. * n ** 2 * (w - a * k ** 2)) / a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIrdeugOoGEJ"
      },
      "source": [
        "решение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3ULNwsGHQ4Fa"
      },
      "outputs": [],
      "source": [
        "def q(x,t):\n",
        "  const = math.exp((x-2*a*k*t-z0)*math.sqrt(mu))\n",
        "  return [math.pow(((4*mu) / ((1/const)+2*l+(l**2-4*mu*nu)*const)), 1/(2*n)) * math.cos(k*x-w*t+th0),\n",
        "  math.pow(((4*mu) / ((1/const)+2*l+(l**2-4*mu*nu)*const)), 1/(2*n)) * math.sin(k*x-w*t+th0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6mPmy7FW508",
        "outputId": "9feac979-c6fa-4a77-a576-c6020994d5f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyDOE in /usr/local/lib/python3.10/dist-packages (0.3.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyDOE) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyDOE) (1.10.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyDOE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ESHqRF_KBd1A"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from collections import OrderedDict #упорядоченный словарь\n",
        "from pyDOE import lhs #функция, выбирающая значения для обучения на них нейросети\n",
        "import numpy as np\n",
        "import time\n",
        "np.random.seed(1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "I-K7dJndUlAB"
      },
      "outputs": [],
      "source": [
        "#можно сменить среду выполнения на gpu и обучение будет происходить быстрее\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def q_tensor(x,t): #тензорная функция q, принимающая на вход тензоры из координат x и t и возвращающая тензоры из u и v\n",
        "  const = torch.exp((x-2*a*k*t-z0)*math.sqrt(mu))\n",
        "  return [torch.pow(((4*mu*const) / (1+2*l*const+(l**2-4*mu*nu)*const**2)), 1/(2*n)) * torch.cos(k*x-w*t+th0),\n",
        "                             torch.pow(((4*mu*const) / (1+2*l*const+(l**2-4*mu*nu)*const**2)), 1/(2*n))*torch.sin(k*x-w*t+th0)]"
      ],
      "metadata": {
        "id": "D_JvKh-eUGN0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SinActivation(torch.nn.Module): #кастомная функция активации - sin\n",
        "    def __init__(self):\n",
        "        super(SinActivation, self).__init__()\n",
        "        return\n",
        "    def forward(self, x):\n",
        "        return torch.sin(x)"
      ],
      "metadata": {
        "id": "Lgsje_Ms8Vmr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INnh3lfZpdTk"
      },
      "source": [
        "нейросеть, в виде которой будет находиться решение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iZq8rBGZUqMc"
      },
      "outputs": [],
      "source": [
        "class DNN(torch.nn.Module):\n",
        "    def __init__(self, layers): #принимает на вход массив целых чисел\n",
        "        super(DNN, self).__init__() #вызывает метод init(почему нельзя сделать это без super?)\n",
        "\n",
        "        self.depth = len(layers) - 1\n",
        "        self.activation = SinActivation #в качестве функции активации используется tanh\n",
        "\n",
        "        layer_list = list() #список с весами и функциями активации для каждого слоя\n",
        "        for i in range(self.depth - 1):\n",
        "            layer_list.append(\n",
        "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1])) #каждые два слоя образуют двудольный граф\n",
        "            )\n",
        "            layer_list.append(('activation_%d' % i, self.activation()))\n",
        "\n",
        "        layer_list.append(\n",
        "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1])) #нельзя сделать в цикле, потому что нет функции активации\n",
        "        )\n",
        "\n",
        "        layerDict = OrderedDict(layer_list) #сделали упорядоченный словарь, чтобы при использовании элементы выдавались в том порядке, в котором были добавлены\n",
        "\n",
        "        # deploy layers\n",
        "        self.layers = torch.nn.Sequential(layerDict) #собственно, задали архитектуру нейросети\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layers(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYh7EE5hpjkO"
      },
      "source": [
        "наконец, PINN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DqcQWmI3Uuy_"
      },
      "outputs": [],
      "source": [
        "class PhysicsInformedNN():\n",
        "    def __init__(self, X_uv, u, v, X_f, layers, lb, ub, a, alpha, beta):\n",
        "\n",
        "        # граничные условия\n",
        "        self.lb = torch.tensor(lb).float().to(device) #левое\n",
        "        self.ub = torch.tensor(ub).float().to(device) #правое\n",
        "\n",
        "        # данные для обучения\n",
        "        self.x_uv = torch.tensor(X_uv[:, 0:1], requires_grad=True).float().to(device) #для граничных условий: (x_uv, t_uv, u, v)\n",
        "        self.t_uv = torch.tensor(X_uv[:, 1:2], requires_grad=True).float().to(device)\n",
        "        self.u = torch.tensor(u).float().to(device)\n",
        "        self.v = torch.tensor(v).float().to(device)\n",
        "        #self.x_f = torch.tensor(X_f[:, 0:1], requires_grad=True).float().to(device) #для уравнения: (x_f, t_f, f=0) (используются переданные сети точки)\n",
        "        #self.t_f = torch.tensor(X_f[:, 1:2], requires_grad=True).float().to(device)\n",
        "        self.x_f = torch.tensor((lb + (ub-lb)*lhs(2, N_f))[:, 0:1], requires_grad=True).float().to(device) #создаём N_f точек для обучения соответствия уравнению. те, что переданы, не используем\n",
        "        self.t_f = torch.tensor((lb + (ub-lb)*lhs(2, N_f))[:, 1:2], requires_grad=True).float().to(device)\n",
        "\n",
        "        #данные для законов сохранения\n",
        "        self.x_fragments = 100 #разбиение при интегрировании по x при фиксированном t\n",
        "        self.t_amount = 10 #в скольких точках t считается интеграл\n",
        "        x_l = np.linspace(x_0, x_1, self.x_fragments).reshape(self.x_fragments,1)\n",
        "        t_l = np.linspace(t_0, t_1, self.t_amount).reshape(self.t_amount,1)\n",
        "        self.x_l = torch.tensor(x_l, requires_grad=True).float().to(device)\n",
        "        self.t_l = torch.tensor(t_l, requires_grad=True).float().to(device)\n",
        "\n",
        "        # числовые коэффициенты в уравнении\n",
        "        self.a = a\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "\n",
        "        #модель\n",
        "        self.layers = layers\n",
        "        self.dnn = DNN(layers).to(device)\n",
        "\n",
        "        # оптимизатор - LBFGS, обучает с точностью до 1e-5 или пока разница в точности уменьшается больше, чем точность float(?)\n",
        "        self.optimizer = torch.optim.LBFGS(\n",
        "            self.dnn.parameters(),\n",
        "            lr=0.0001,\n",
        "            max_iter=50000,\n",
        "            max_eval=50000,\n",
        "            history_size=50,\n",
        "            tolerance_grad=1e-5,\n",
        "            tolerance_change=1.0 * np.finfo(float).eps,\n",
        "            #line_search_fn=\"strong_wolfe\"\n",
        "        )\n",
        "\n",
        "        self.adam = torch.optim.Adam(\n",
        "          self.dnn.parameters(),\n",
        "          lr=0.005,\n",
        "          betas=(0.9, 0.999),\n",
        "          eps=1e-08,\n",
        "          weight_decay=0,\n",
        "          amsgrad=False,\n",
        "          foreach=None,\n",
        "          maximize=False,\n",
        "          capturable=False,\n",
        "          differentiable=False,\n",
        "          fused=None)\n",
        "\n",
        "        self.iter = 0\n",
        "\n",
        "    def net_uv(self, x, t): # вывод модели\n",
        "        u = self.dnn(torch.cat([x, t], dim=1))[:,0:1]\n",
        "        v = self.dnn(torch.cat([x, t], dim=1))[:,1:2]\n",
        "        return u, v\n",
        "\n",
        "    def net_f(self, x, t): #вывод  функции\n",
        "        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n",
        "        u, v = self.net_uv(x, t)\n",
        "\n",
        "        u_t = torch.autograd.grad(\n",
        "            u, t,\n",
        "            grad_outputs=torch.ones_like(u),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0] #производая по t\n",
        "        u_x = torch.autograd.grad(\n",
        "            u, x,\n",
        "            grad_outputs=torch.ones_like(u),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0] #производная по x\n",
        "        u_xx = torch.autograd.grad(\n",
        "            u_x, x,\n",
        "            grad_outputs=torch.ones_like(u_x),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0] #вторая призводная по x\n",
        "\n",
        "        v_t = torch.autograd.grad(\n",
        "            v, t,\n",
        "            grad_outputs=torch.ones_like(v),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0] #производая по t\n",
        "        v_x = torch.autograd.grad(\n",
        "            v, x,\n",
        "            grad_outputs=torch.ones_like(v),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0] #производная по x\n",
        "        v_xx = torch.autograd.grad(\n",
        "            v_x, x,\n",
        "            grad_outputs=torch.ones_like(v_x),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0] #вторая призводная по x\n",
        "\n",
        "        f_u = -v_t + (self.a*u_xx + self.alpha*u*(u**2+v**2) - self.beta*u*(u**2+v**2)**2) #это и есть действительная и коплексная части исходного уравнения\n",
        "        f_v = u_t + (self.a*v_xx + self.alpha*v*(u**2+v**2) - self.beta*v*(u**2+v**2)**2) #они получены путём подстановки q=u+i*v в него\n",
        "        return f_u, f_v\n",
        "\n",
        "    def first_law(self): #реализация первого закона сохранения\n",
        "        x_step=(self.x_l[1]-self.x_l[0]).item()\n",
        "        initial_integral=0\n",
        "        for j in range(0,self.x_fragments): #считаем интеграл по x в точке t_0\n",
        "          u, v = self.net_uv(self.x_l[j:j+1], self.t_l[0:1]) #интеграл считается от вывода нейросети\n",
        "          #u, v = q_tensor(self.x_l[j:j+1], self.t_l[0:1]) #интеграл считается от q(x,t), ведь она по сути дана в t_0\n",
        "          initial_integral += (u**2 + v**2) * x_step\n",
        "\n",
        "        variation=np.zeros(self.t_amount)\n",
        "        for i in range(1,self.t_amount): #сравниваем с интегралом по x в других точках\n",
        "          integral=0\n",
        "          for j in range(0,self.x_fragments):\n",
        "            u, v = self.net_uv(self.x_l[j:j+1], self.t_l[i:i+1])\n",
        "            integral += (u**2 + v**2) * x_step\n",
        "          variation[i]=(initial_integral-integral)**2\n",
        "\n",
        "        return variation.mean(axis=0) #возвращаем средний квадрат отклонения\n",
        "\n",
        "    def second_law(self): #реализация второго закона сохранения\n",
        "        x_step=(self.x_l[1]-self.x_l[0]).item()\n",
        "        initial_integral=0\n",
        "        for j in range(0,self.x_fragments): #считаем интеграл по x в t_0\n",
        "          u, v = self.net_uv(self.x_l[j:j+1], self.t_l[0:1]) #интеграл считается от вывода нейросети\n",
        "          #u, v = q_tensor(self.x_l[j:j+1], self.t_l[0:1]) #интеграл считается от q(x,t)\n",
        "          u_x = torch.autograd.grad(u, self.x_l, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0]\n",
        "          v_x = torch.autograd.grad(v, self.x_l, grad_outputs=torch.ones_like(v), retain_graph=True, create_graph=True)[0]\n",
        "          initial_integral += ((v_x[j] * u - u_x[j] * v) * x_step)\n",
        "\n",
        "        variation=np.zeros(self.t_amount)\n",
        "        for i in range(1,self.t_amount): #сравниваем с интегралом по x в других точках\n",
        "          integral=0\n",
        "          for j in range(0,self.x_fragments):\n",
        "            u, v = self.net_uv(self.x_l[j:j+1], self.t_l[i:i+1])\n",
        "            u_x = torch.autograd.grad(u, self.x_l, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0] #почему-то градиент по self.x_l[j:j+1] посчитать не получается\n",
        "            v_x = torch.autograd.grad(v, self.x_l, grad_outputs=torch.ones_like(v), retain_graph=True, create_graph=True)[0] #поэтому вычисляется градиент по self.x_l и берётся j элемент\n",
        "            integral += ((v_x[j] * u - u_x[j] * v) * x_step)\n",
        "          variation[i]=(initial_integral-integral)**2\n",
        "\n",
        "        return variation.mean(axis=0) #опять возвращаем средний квадрат отклонения\n",
        "\n",
        "    def loss_func(self): #функция потерь\n",
        "        self.optimizer.zero_grad() #обнуляет градиенты\n",
        "\n",
        "        u_pred, v_pred = self.net_uv(self.x_uv, self.t_uv)\n",
        "        f_u_pred, f_v_pred = self.net_f(self.x_f, self.t_f)\n",
        "        loss_uv = torch.mean(((self.u - u_pred) ** 2 + (self.v - v_pred) ** 2)/2) #средний квадрат всех отклонений от начальных условий\n",
        "        loss_f = torch.mean((f_u_pred ** 2 + f_v_pred ** 2)/2) #средний квадрат всех отклонений от условия\n",
        "\n",
        "        #loss_fl = self.first_law()\n",
        "        #loss_sl = self.second_law()\n",
        "\n",
        "        #loss = loss_uv + loss_f + loss_fl + loss_sl\n",
        "        loss = loss_uv + loss_f\n",
        "\n",
        "        loss.backward()\n",
        "        self.iter += 1\n",
        "        if self.iter % 100 == 0:\n",
        "            print(\n",
        "                'Iter %d, Loss: %.5e, Loss_uv: %.5e, Loss_f: %.5e' % (self.iter, loss.item(), loss_uv.item(), loss_f.item())\n",
        "            )\n",
        "            #print('loss_fl: %.5e' %loss_fl.item())\n",
        "            #print('loss_sl: %.5e' %loss_sl.item())\n",
        "            u_pred_current, v_pred_current, f_u_pred_current, f_v_pred_current = self.predict(X_star)\n",
        "            mse_q_current = (((u_star**2+v_star**2)**0.5 - (u_pred_current**2+v_pred_current**2)**0.5)**2).mean(axis=0).item() #средний квадрат разности модулей\n",
        "            mse_q_array.append(mse_q_current) #запоминаем mse_q в текущей стадии обучения чтобы потом построить график\n",
        "            loss_array.append(loss) #loss тоже запоминаем для графика\n",
        "            iter_array.append(self.iter)\n",
        "\n",
        "        if self.iter % 10 == 0: #каждые 10 итераций генерируем новые точки для обучения\n",
        "          random_points = lb + (ub-lb)*lhs(2, N_f)\n",
        "          self.x_f = torch.tensor(random_points[:, 0:1], requires_grad=True).float().to(device)\n",
        "          self.t_f = torch.tensor(random_points[:, 1:2], requires_grad=True).float().to(device)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def train(self): #обучение\n",
        "        print('training started')\n",
        "        print('100000 iterations of ADAM:')\n",
        "        self.dnn.train()\n",
        "        for i in range(100000): #во время тренировки производится 100000 шагов adam, а дальше запускается lbfgs\n",
        "            if i % 100 == 0: self.adam.param_groups[0]['lr'] = 0.997*self.adam.param_groups[0]['lr'] #экспоненциальное уменьшение шага каждые 100 шагов\n",
        "            self.adam.step(self.loss_func)\n",
        "        print('LBFGS:')\n",
        "        self.optimizer.step(self.loss_func)\n",
        "\n",
        "\n",
        "    def predict(self, X): #вывод нейросети и функции на входных данных\n",
        "        x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n",
        "        t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n",
        "\n",
        "        self.dnn.eval()\n",
        "        u, v = self.net_uv(x, t)\n",
        "        f_u, f_v = self.net_f(x, t)\n",
        "        u = u.detach().cpu().numpy()\n",
        "        v = v.detach().cpu().numpy()\n",
        "        f_u = f_u.detach().cpu().numpy()\n",
        "        f_v = f_v.detach().cpu().numpy()\n",
        "        return u, v, f_u, f_v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFMEDI4_pqZL"
      },
      "source": [
        "данные для обучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XdohH47tU0cf"
      },
      "outputs": [],
      "source": [
        "N_u = 200 # число точек, обучающих принимать граничные значения\n",
        "N_f = 30000 # число точек, обучающих удовлетворять уравнению\n",
        "x_parts = 1000 #число частей, на которые разбивается отрезок x\n",
        "t_parts = 100 #число частей, на которые разбивается отрезок t\n",
        "layers = [2, 100, 100, 100, 100, 2] #2 входа, 2 выхода и 4 слоя по 100 нейронов\n",
        "\n",
        "x_0=-85\n",
        "x_1=85\n",
        "t_0=0\n",
        "t_1=50\n",
        "x = np.linspace(x_0, x_1, x_parts)\n",
        "t = np.linspace(t_0, t_1, t_parts)\n",
        "X, T = np.meshgrid(x, t)\n",
        "Exact_u=np.zeros((t_parts,x_parts))\n",
        "Exact_v=np.zeros((t_parts,x_parts))\n",
        "for i in range(0,t_parts):\n",
        "  for j in range(0,x_parts):\n",
        "    Exact_u[i][j]=q(X[i][j],T[i][j])[0]\n",
        "    Exact_v[i][j]=q(X[i][j],T[i][j])[1]\n",
        "\n",
        "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
        "u_star = Exact_u.flatten()[:,None]\n",
        "v_star = Exact_v.flatten()[:,None]\n",
        "\n",
        "# границы области\n",
        "lb = X_star.min(0)\n",
        "ub = X_star.max(0)\n",
        "\n",
        "# для обучения берём только данные на границах области(граничные условия по сути)\n",
        "xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T)) #(x,t_0)\n",
        "uu1 = Exact_u[0:1,:].T\n",
        "vv1 = Exact_v[0:1,:].T\n",
        "xx2 = np.hstack((X[:,0:1], T[:,0:1])) #(x_0,t)\n",
        "uu2 = np.zeros((t_parts,1)) #граничные условия поставил 0 в соответствии с требованиями\n",
        "vv2 = np.zeros((t_parts,1))\n",
        "xx3 = np.hstack((X[:,-1:], T[:,-1:])) #(x_1,t)\n",
        "uu3 = np.zeros((t_parts,1)) #тут тоже\n",
        "vv3 = np.zeros((t_parts,1))\n",
        "\n",
        "X_uv_train = np.vstack([xx1, xx2, xx3]) #данные для тренировки в точках на границе\n",
        "u_train = np.vstack([uu1, uu2, uu3])\n",
        "v_train = np.vstack([vv1, vv2, vv3])\n",
        "X_f_train = lb + (ub-lb)*lhs(2, N_f) #данные для тренировки в случайных точкам из области\n",
        "X_f_train = np.vstack((X_f_train, X_uv_train)) #добавим к ним ещё и точки на границе, там f тоже 0\n",
        "\n",
        "idx = np.random.choice(X_uv_train.shape[0], N_u, replace=False) #выберем из точек на границе только N_u\n",
        "X_uv_train = X_uv_train[idx, :]\n",
        "u_train = u_train[idx,:]\n",
        "v_train = v_train[idx,:]\n",
        "\n",
        "mse_q_array = [] #массив с mse_q для каждой сотой итерации обучения\n",
        "loss_array = [] #аналогичный массив с loss\n",
        "iter_array = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwUjv7emVVqP"
      },
      "source": [
        "Обучение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "McQX7sQ5U9DB"
      },
      "outputs": [],
      "source": [
        "model = PhysicsInformedNN(X_uv_train, u_train, v_train, X_f_train, layers, lb, ub, a, alpha, beta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zAUFaIJVGZd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "483433aa-bc33-48de-ce2f-dd02a59f1451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training started\n",
            "100000 iterations of ADAM:\n",
            "Iter 100, Loss: 2.76119e-03, Loss_uv: 2.72184e-03, Loss_f: 3.93514e-05\n",
            "Iter 200, Loss: 1.79584e-03, Loss_uv: 1.50929e-03, Loss_f: 2.86554e-04\n",
            "Iter 300, Loss: 1.02782e-03, Loss_uv: 6.19835e-04, Loss_f: 4.07982e-04\n",
            "Iter 400, Loss: 7.20958e-04, Loss_uv: 3.43603e-04, Loss_f: 3.77355e-04\n",
            "Iter 500, Loss: 5.44713e-04, Loss_uv: 2.27739e-04, Loss_f: 3.16974e-04\n",
            "Iter 600, Loss: 4.20582e-04, Loss_uv: 1.63972e-04, Loss_f: 2.56610e-04\n",
            "Iter 700, Loss: 3.15006e-04, Loss_uv: 1.17210e-04, Loss_f: 1.97796e-04\n",
            "Iter 800, Loss: 2.78062e-04, Loss_uv: 9.54647e-05, Loss_f: 1.82597e-04\n",
            "Iter 900, Loss: 2.25595e-04, Loss_uv: 7.67770e-05, Loss_f: 1.48818e-04\n",
            "Iter 1000, Loss: 2.19321e-04, Loss_uv: 6.69294e-05, Loss_f: 1.52391e-04\n",
            "Iter 1100, Loss: 2.17307e-04, Loss_uv: 6.40481e-05, Loss_f: 1.53259e-04\n",
            "Iter 1200, Loss: 1.85370e-04, Loss_uv: 5.48234e-05, Loss_f: 1.30546e-04\n",
            "Iter 1300, Loss: 1.88627e-04, Loss_uv: 5.95775e-05, Loss_f: 1.29049e-04\n",
            "Iter 1400, Loss: 1.72403e-04, Loss_uv: 4.87506e-05, Loss_f: 1.23653e-04\n",
            "Iter 1500, Loss: 1.82618e-04, Loss_uv: 5.50751e-05, Loss_f: 1.27542e-04\n",
            "Iter 1600, Loss: 1.45317e-04, Loss_uv: 4.12084e-05, Loss_f: 1.04108e-04\n",
            "Iter 1700, Loss: 1.71041e-04, Loss_uv: 4.39804e-05, Loss_f: 1.27060e-04\n",
            "Iter 1800, Loss: 1.72417e-04, Loss_uv: 4.47952e-05, Loss_f: 1.27622e-04\n",
            "Iter 1900, Loss: 1.46147e-04, Loss_uv: 3.89430e-05, Loss_f: 1.07204e-04\n",
            "Iter 2000, Loss: 1.41495e-04, Loss_uv: 4.02756e-05, Loss_f: 1.01219e-04\n",
            "Iter 2100, Loss: 1.86206e-04, Loss_uv: 7.30975e-05, Loss_f: 1.13109e-04\n",
            "Iter 2200, Loss: 1.51549e-04, Loss_uv: 4.61378e-05, Loss_f: 1.05411e-04\n",
            "Iter 2300, Loss: 1.30930e-04, Loss_uv: 3.03924e-05, Loss_f: 1.00537e-04\n",
            "Iter 2400, Loss: 1.34543e-04, Loss_uv: 3.30115e-05, Loss_f: 1.01531e-04\n",
            "Iter 2500, Loss: 1.32686e-04, Loss_uv: 2.86272e-05, Loss_f: 1.04059e-04\n",
            "Iter 2600, Loss: 1.23804e-04, Loss_uv: 2.68800e-05, Loss_f: 9.69236e-05\n",
            "Iter 2700, Loss: 1.26285e-04, Loss_uv: 2.59514e-05, Loss_f: 1.00334e-04\n",
            "Iter 2800, Loss: 1.42870e-04, Loss_uv: 5.11887e-05, Loss_f: 9.16817e-05\n",
            "Iter 2900, Loss: 1.22013e-04, Loss_uv: 3.72488e-05, Loss_f: 8.47640e-05\n",
            "Iter 3000, Loss: 1.10143e-04, Loss_uv: 3.56609e-05, Loss_f: 7.44817e-05\n",
            "Iter 3100, Loss: 1.09828e-04, Loss_uv: 2.15251e-05, Loss_f: 8.83029e-05\n",
            "Iter 3200, Loss: 1.19262e-04, Loss_uv: 2.69840e-05, Loss_f: 9.22784e-05\n",
            "Iter 3300, Loss: 1.04268e-04, Loss_uv: 1.95878e-05, Loss_f: 8.46804e-05\n",
            "Iter 3400, Loss: 1.16763e-04, Loss_uv: 3.09777e-05, Loss_f: 8.57856e-05\n",
            "Iter 3500, Loss: 8.87250e-05, Loss_uv: 1.81465e-05, Loss_f: 7.05785e-05\n",
            "Iter 3600, Loss: 1.08864e-04, Loss_uv: 2.09202e-05, Loss_f: 8.79440e-05\n",
            "Iter 3700, Loss: 1.59963e-04, Loss_uv: 7.03435e-05, Loss_f: 8.96198e-05\n",
            "Iter 3800, Loss: 1.05644e-04, Loss_uv: 2.32325e-05, Loss_f: 8.24115e-05\n",
            "Iter 3900, Loss: 1.05210e-04, Loss_uv: 2.39434e-05, Loss_f: 8.12671e-05\n",
            "Iter 4000, Loss: 9.68624e-05, Loss_uv: 1.98734e-05, Loss_f: 7.69890e-05\n",
            "Iter 4100, Loss: 8.90263e-05, Loss_uv: 1.68506e-05, Loss_f: 7.21757e-05\n",
            "Iter 4200, Loss: 9.26445e-05, Loss_uv: 1.74491e-05, Loss_f: 7.51954e-05\n",
            "Iter 4300, Loss: 9.46506e-05, Loss_uv: 1.84084e-05, Loss_f: 7.62422e-05\n",
            "Iter 4400, Loss: 1.18951e-04, Loss_uv: 2.12729e-05, Loss_f: 9.76777e-05\n",
            "Iter 4500, Loss: 8.53963e-05, Loss_uv: 1.47531e-05, Loss_f: 7.06431e-05\n",
            "Iter 4600, Loss: 9.34077e-05, Loss_uv: 2.26237e-05, Loss_f: 7.07840e-05\n",
            "Iter 4700, Loss: 8.21853e-05, Loss_uv: 1.79551e-05, Loss_f: 6.42302e-05\n",
            "Iter 4800, Loss: 1.28322e-04, Loss_uv: 5.53274e-05, Loss_f: 7.29942e-05\n",
            "Iter 4900, Loss: 9.23413e-05, Loss_uv: 2.14663e-05, Loss_f: 7.08751e-05\n",
            "Iter 5000, Loss: 9.10537e-05, Loss_uv: 1.97828e-05, Loss_f: 7.12709e-05\n",
            "Iter 5100, Loss: 1.04575e-04, Loss_uv: 3.73580e-05, Loss_f: 6.72169e-05\n",
            "Iter 5200, Loss: 9.36393e-05, Loss_uv: 2.31776e-05, Loss_f: 7.04617e-05\n",
            "Iter 5300, Loss: 8.50479e-05, Loss_uv: 2.43437e-05, Loss_f: 6.07043e-05\n",
            "Iter 5400, Loss: 1.14874e-04, Loss_uv: 4.69151e-05, Loss_f: 6.79587e-05\n",
            "Iter 5500, Loss: 7.65049e-05, Loss_uv: 1.36693e-05, Loss_f: 6.28356e-05\n",
            "Iter 5600, Loss: 8.46456e-05, Loss_uv: 1.52559e-05, Loss_f: 6.93897e-05\n",
            "Iter 5700, Loss: 7.82641e-05, Loss_uv: 1.58285e-05, Loss_f: 6.24356e-05\n",
            "Iter 5800, Loss: 7.78139e-05, Loss_uv: 1.95951e-05, Loss_f: 5.82188e-05\n",
            "Iter 5900, Loss: 8.50004e-05, Loss_uv: 2.40816e-05, Loss_f: 6.09188e-05\n",
            "Iter 6000, Loss: 7.10431e-05, Loss_uv: 1.44123e-05, Loss_f: 5.66308e-05\n",
            "Iter 6100, Loss: 1.01822e-04, Loss_uv: 3.09819e-05, Loss_f: 7.08406e-05\n",
            "Iter 6200, Loss: 7.13357e-05, Loss_uv: 1.71594e-05, Loss_f: 5.41763e-05\n",
            "Iter 6300, Loss: 7.75195e-05, Loss_uv: 1.51935e-05, Loss_f: 6.23261e-05\n",
            "Iter 6400, Loss: 6.96662e-05, Loss_uv: 1.38551e-05, Loss_f: 5.58112e-05\n",
            "Iter 6500, Loss: 7.19726e-05, Loss_uv: 1.68457e-05, Loss_f: 5.51268e-05\n",
            "Iter 6600, Loss: 7.24634e-05, Loss_uv: 1.48225e-05, Loss_f: 5.76409e-05\n",
            "Iter 6700, Loss: 7.11926e-05, Loss_uv: 1.46630e-05, Loss_f: 5.65297e-05\n",
            "Iter 6800, Loss: 7.37541e-05, Loss_uv: 1.86346e-05, Loss_f: 5.51195e-05\n",
            "Iter 6900, Loss: 7.24684e-05, Loss_uv: 1.52781e-05, Loss_f: 5.71903e-05\n",
            "Iter 7000, Loss: 7.41552e-05, Loss_uv: 1.37466e-05, Loss_f: 6.04087e-05\n",
            "Iter 7100, Loss: 7.53221e-05, Loss_uv: 1.53403e-05, Loss_f: 5.99818e-05\n",
            "Iter 7200, Loss: 8.48736e-05, Loss_uv: 1.76725e-05, Loss_f: 6.72011e-05\n",
            "Iter 7300, Loss: 7.00394e-05, Loss_uv: 1.31974e-05, Loss_f: 5.68420e-05\n",
            "Iter 7400, Loss: 5.95007e-05, Loss_uv: 1.23263e-05, Loss_f: 4.71745e-05\n",
            "Iter 7500, Loss: 7.31370e-05, Loss_uv: 1.34185e-05, Loss_f: 5.97185e-05\n",
            "Iter 7600, Loss: 7.48333e-05, Loss_uv: 1.34075e-05, Loss_f: 6.14258e-05\n",
            "Iter 7700, Loss: 7.54431e-05, Loss_uv: 2.32734e-05, Loss_f: 5.21697e-05\n",
            "Iter 7800, Loss: 6.11933e-05, Loss_uv: 1.19324e-05, Loss_f: 4.92609e-05\n",
            "Iter 7900, Loss: 6.92022e-05, Loss_uv: 1.29626e-05, Loss_f: 5.62395e-05\n",
            "Iter 8000, Loss: 7.49146e-05, Loss_uv: 1.87250e-05, Loss_f: 5.61896e-05\n",
            "Iter 8100, Loss: 6.32621e-05, Loss_uv: 1.33018e-05, Loss_f: 4.99603e-05\n",
            "Iter 8200, Loss: 6.43989e-05, Loss_uv: 1.31892e-05, Loss_f: 5.12096e-05\n",
            "Iter 8300, Loss: 6.08737e-05, Loss_uv: 1.41031e-05, Loss_f: 4.67706e-05\n",
            "Iter 8400, Loss: 8.36882e-05, Loss_uv: 2.10308e-05, Loss_f: 6.26574e-05\n",
            "Iter 8500, Loss: 7.31373e-05, Loss_uv: 1.38415e-05, Loss_f: 5.92958e-05\n",
            "Iter 8600, Loss: 7.20301e-05, Loss_uv: 1.51242e-05, Loss_f: 5.69059e-05\n",
            "Iter 8700, Loss: 7.62605e-05, Loss_uv: 1.53745e-05, Loss_f: 6.08860e-05\n",
            "Iter 8800, Loss: 5.73022e-05, Loss_uv: 1.05983e-05, Loss_f: 4.67039e-05\n",
            "Iter 8900, Loss: 7.02849e-05, Loss_uv: 2.29896e-05, Loss_f: 4.72953e-05\n",
            "Iter 9000, Loss: 5.59421e-05, Loss_uv: 1.19348e-05, Loss_f: 4.40074e-05\n",
            "Iter 9100, Loss: 5.23254e-05, Loss_uv: 1.08063e-05, Loss_f: 4.15191e-05\n",
            "Iter 9200, Loss: 6.44742e-05, Loss_uv: 1.91726e-05, Loss_f: 4.53016e-05\n",
            "Iter 9300, Loss: 5.80377e-05, Loss_uv: 1.00960e-05, Loss_f: 4.79417e-05\n",
            "Iter 9400, Loss: 5.49028e-05, Loss_uv: 1.05671e-05, Loss_f: 4.43357e-05\n",
            "Iter 9500, Loss: 6.98907e-05, Loss_uv: 2.77761e-05, Loss_f: 4.21146e-05\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "#штука сверху выведет итоговое время выполнения\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(np.array(iter_array), np.array(loss_array), color = \"blue\")\n",
        "plt.title('loss(iter)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pZrbXMDJHAPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.array(iter_array), np.array(mse_q_array), color = \"blue\")\n",
        "plt.title('mse_q(iter)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KP6Mcf2VnITC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd #сохраняем в таблицу статистику обучения\n",
        "df = pd.DataFrame([np.array(mse_q_array), np.array(loss_array), np.array(iter_array)], index=['mse_q', 'loss', 'iter'])\n",
        "df.to_excel(\"stats.xlsx\")"
      ],
      "metadata": {
        "id": "QwY2khYl0MD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOGdVkXCVIlH"
      },
      "outputs": [],
      "source": [
        "u_pred, v_pred, f_u_pred, f_v_pred = model.predict(X_star) #предсказания модели в точках, в которых известно численное решение\n",
        "\n",
        "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2) #относительная точность найденной функции u\n",
        "error_v = np.linalg.norm(v_star-v_pred,2)/np.linalg.norm(v_star,2) # и то же самое для v\n",
        "error_f = np.linalg.norm(f_u_pred+f_v_pred,2) #абсолютная точность выполнения условий уравнения f=0\n",
        "print('Rel error u: %e, Rel error v: %e, Abs error f: %e' % (error_u, error_v, error_f)) #довольно неплохие результаты, учитывая, что вектор f_pred имеет длину x_parts*t_parts=100000\n",
        "\n",
        "mse_u = ((u_star-u_pred)**2).mean(axis=0).item()\n",
        "mse_v = ((v_star-v_pred)**2).mean(axis=0).item()\n",
        "mse_q = (((u_star**2+v_star**2)**0.5 - (u_pred**2+v_pred**2)**0.5)**2).mean(axis=0).item() #средний квадрат разности модулей\n",
        "mse_f_u = ((f_u_pred)**2).mean(axis=0).item()\n",
        "mse_f_v = ((f_v_pred)**2).mean(axis=0).item()\n",
        "print('MSE_u: %e, MSE_v: %e, MSE_q: %e, MSE_f_u: %e, MSE_f_v: %e' %(mse_u, mse_v, mse_q, mse_f_u, mse_f_v)) #среднеквадратичное отклонение от известного решения во всех точках сетки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7y79Xiye9UE"
      },
      "source": [
        "Визуализация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FG5HJmUQOmH7"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "u_min, u_max = -np.abs(u_pred).max(), np.abs(u_pred).max()\n",
        "c = ax.pcolormesh(T, X, u_pred.reshape((t_parts, x_parts)), cmap='RdBu', vmin=u_min, vmax=u_max)\n",
        "ax.set_title('u_pred(x,t)')\n",
        "ax.axis([t_0, t_1, x_0, x_1])\n",
        "fig.colorbar(c, ax=ax)\n",
        "#plt.scatter(X_uv_train[:,1], X_uv_train[:,0], s=7, color='black')\n",
        "#plt.scatter(X_f_train[:,1], X_f_train[:,0], s=3, color='black', alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "u_min, u_max = -np.abs(Exact_u).max(), np.abs(Exact_u).max()\n",
        "c = ax.pcolormesh(T, X, Exact_u, cmap='RdBu', vmin=u_min, vmax=u_max)\n",
        "ax.set_title('u_truth(x,t)')\n",
        "ax.axis([t_0, t_1, x_0, x_1])\n",
        "fig.colorbar(c, ax=ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GPMVVPdV-CW"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "v_min, v_max = -np.abs(v_pred).max(), np.abs(v_pred).max()\n",
        "c = ax.pcolormesh(T, X, v_pred.reshape((t_parts, x_parts)), cmap='RdBu', vmin=v_min, vmax=v_max)\n",
        "ax.set_title('v_pred(x,t)')\n",
        "ax.axis([t_0, t_1, x_0, x_1])\n",
        "fig.colorbar(c, ax=ax)\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "v_min, v_max = -np.abs(Exact_v).max(), np.abs(Exact_v).max()\n",
        "c = ax.pcolormesh(T, X, Exact_v, cmap='RdBu', vmin=v_min, vmax=v_max)\n",
        "ax.set_title('v_truth(x,t)')\n",
        "ax.axis([t_0, t_1, x_0, x_1])\n",
        "fig.colorbar(c, ax=ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(t_0,t_1+1, 10):\n",
        "  current_row = int(t_parts*(t-t_0)/(t_1-t_0))\n",
        "  if t==t_1: current_row=-1 #если смотрим значения в t_1, то ясно что это последняя колонка\n",
        "  plt.plot(X[current_row, :], u_pred.reshape((t_parts, x_parts))[current_row,:], color = \"red\") #предсказанное решение\n",
        "  plt.plot(X[current_row, :], Exact_u[current_row,:], color = \"green\") #аналитическое решение\n",
        "  plt.title('u_pred(x,t=%.2f)'%t)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "VJGf_wB1Afx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(t_0,t_1+1, 10):\n",
        "  current_row = int(t_parts*(t-t_0)/(t_1-t_0))\n",
        "  if t==t_1: current_row=-1 #если смотрим значения в t_1, то ясно что это последняя колонка\n",
        "  plt.plot(X[current_row, :], v_pred.reshape((t_parts, x_parts))[current_row,:], color = \"red\")\n",
        "  plt.plot(X[current_row, :], Exact_v[current_row,:], color = \"green\")\n",
        "  plt.title('v_pred(x,t=%.2f)'%t)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "VpfujHIVAmoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "err = ((u_star-u_pred)**2 + (v_star-v_pred)**2)**0.5 #модуль разности q_truth и q_pred\n",
        "fig, ax = plt.subplots()\n",
        "e_min, e_max = err.min(), err.max()\n",
        "c = ax.pcolormesh(T, X, err.reshape((t_parts, x_parts)), cmap='binary', vmin=e_min, vmax=e_max)\n",
        "ax.set_title('|q_truth-q_pred|(x,t)')\n",
        "ax.axis([t_0, t_1, x_0, x_1])\n",
        "fig.colorbar(c, ax=ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R0W6HXlPe7ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "err = (((u_star**2+v_star**2)**0.5 - (u_pred**2+v_pred**2)**0.5)**2)\n",
        "fig, ax = plt.subplots()\n",
        "e_min, e_max = err.min(), err.max()\n",
        "c = ax.pcolormesh(T, X, err.reshape((t_parts, x_parts)), cmap='binary', vmin=e_min, vmax=e_max)\n",
        "ax.set_title('|q_truth|-|q_pred| (x,t)')\n",
        "ax.axis([t_0, t_1, x_0, x_1])\n",
        "fig.colorbar(c, ax=ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U8UNsXFcXcEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверка законов сохранения"
      ],
      "metadata": {
        "id": "A70zjI84sMw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_fragments = 100 #разбиение при интегрировании по x при фиксированном t\n",
        "t_amount = 10 #в скольких точках t считается интеграл\n",
        "x_l = np.linspace(x_0, x_1, x_fragments).reshape(x_fragments,1)\n",
        "t_l = np.linspace(t_0, t_1, t_amount).reshape(t_amount,1)\n",
        "X_l = torch.tensor(x_l, requires_grad=True).float().to(device)\n",
        "T_l = torch.tensor(t_l, requires_grad=True).float().to(device)\n",
        "\n",
        "#реализация первого закона сохранения\n",
        "x_step=(X_l[1]-X_l[0]).item()\n",
        "initial_integral=0\n",
        "for j in range(0,x_fragments): #считаем интеграл по x в точке t_0\n",
        "  u, v = model.net_uv(X_l[j:j+1], T_l[0:1])\n",
        "  initial_integral += (u**2 + v**2) * x_step\n",
        "\n",
        "first_variation=np.zeros(t_amount)\n",
        "for i in range(1,t_amount): #сравниваем с интегралом по x в других точках\n",
        "  integral=0\n",
        "  for j in range(0,x_fragments):\n",
        "    u, v =model.net_uv(X_l[j:j+1], T_l[i:i+1])\n",
        "    integral += (u**2 + v**2) * x_step\n",
        "  first_variation[i]=(initial_integral-integral)**2\n",
        "MSE_fl = first_variation.mean(axis=0).item() #возвращаем средний квадрат отклонения\n",
        "\n",
        "\n",
        "initial_integral=0\n",
        "for j in range(0,x_fragments): #считаем интеграл по x в t_0\n",
        "  u, v = model.net_uv(X_l[j:j+1], T_l[0:1])\n",
        "  u_x = torch.autograd.grad(u, X_l, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0]\n",
        "  v_x = torch.autograd.grad(v, X_l, grad_outputs=torch.ones_like(v), retain_graph=True, create_graph=True)[0]\n",
        "  initial_integral += ((v_x[j] * u - u_x[j] * v) * x_step)\n",
        "\n",
        "second_variation=np.zeros(t_amount)\n",
        "for i in range(1,t_amount): #сравниваем с интегралом по x в других точках\n",
        "  integral=0\n",
        "  for j in range(0,x_fragments):\n",
        "    u, v = model.net_uv(X_l[j:j+1], T_l[i:i+1])\n",
        "    u_x = torch.autograd.grad(u, X_l, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0] #почему-то градиент по self.x_l[j:j+1] посчитать не получается\n",
        "    v_x = torch.autograd.grad(v, X_l, grad_outputs=torch.ones_like(v), retain_graph=True, create_graph=True)[0] #поэтому вычисляется градиент по self.x_l и берётся j элемент\n",
        "    integral += ((v_x[j] * u - u_x[j] * v) * x_step)\n",
        "  second_variation[i]=(initial_integral-integral)**2\n",
        "MSE_sl = second_variation.mean(axis=0).item() #опять возвращаем средний квадрат отклонения\n",
        "\n",
        "\n",
        "print('First law MSE: %e, Second law MSE: %e' %(MSE_fl, MSE_sl))"
      ],
      "metadata": {
        "id": "y42n1oFPsSsp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}